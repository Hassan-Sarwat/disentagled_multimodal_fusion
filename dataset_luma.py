"""
LUMA Dataset Loader for Multimodal Uncertainty Quantification

This module provides PyTorch Dataset and DataLoader functionality for the LUMA dataset,
which contains audio, text, and image modalities.

LUMA consists of:
- Audio: wav files of people pronouncing class labels
- Text: Short passages about class labels (generated by LLMs)
- Image: Images from CIFAR-10/100 subset

Dataset structure after compilation:
- 42 training classes, 8 OOD classes
- 500 samples per class (train), 100 samples per class (test)
"""

import os
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
from typing import Tuple, List, Optional, Dict
import warnings

import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchvision.transforms as transforms
from PIL import Image

# Try to import transformers for text encoding
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    warnings.warn("transformers library not available. Text encoding will use simple tokenization.")


class LUMADataset(Dataset):
    """
    LUMA Multimodal Dataset for Uncertainty Quantification.
    
    Loads and provides access to audio, text, and image modalities.
    """
    
    def __init__(
        self,
        data_path: str,
        split: str = 'train',
        audio_config: Optional[Dict] = None,
        text_config: Optional[Dict] = None,
        image_config: Optional[Dict] = None,
        use_ood: bool = False,
    ):
        """
        Initialize LUMA dataset.
        
        Args:
            data_path: Path to compiled LUMA dataset
            split: 'train' or 'test'
            audio_config: Audio processing configuration
            text_config: Text processing configuration
            image_config: Image processing configuration
            use_ood: Whether to include out-of-distribution samples
        """
        self.data_path = Path(data_path)
        self.split = split
        self.use_ood = use_ood
        
        # Default configurations
        self.audio_config = audio_config or {
            'sample_rate': 16000,
            'max_length': 3.0,
            'n_mfcc': 40,
            'use_mfcc': True,
        }
        
        self.text_config = text_config or {
            'max_length': 128,
            'model_name': 'bert-base-uncased',
            'use_pretrained': TRANSFORMERS_AVAILABLE,
        }
        
        self.image_config = image_config or {
            'size': (32, 32),
            'normalize': True,
        }
        
        # Load dataset metadata
        self._load_metadata()
        
        # Load data indices and labels
        self._load_data_indices()
        
        # Initialize text tokenizer if available
        if self.text_config['use_pretrained'] and TRANSFORMERS_AVAILABLE:
            self.tokenizer = AutoTokenizer.from_pretrained(self.text_config['model_name'])
        else:
            self.tokenizer = None
        
        # Setup image transforms
        self._setup_image_transforms()
        
        print(f"Loaded LUMA {split} dataset: {len(self)} samples, {self.num_classes} classes")
    
    def _load_metadata(self):
        """Load dataset metadata."""
        metadata_path = self.data_path / 'metadata.yaml'
        
        if metadata_path.exists():
            import yaml
            with open(metadata_path, 'r') as f:
                metadata = yaml.safe_load(f)
            
            self.num_classes = metadata.get('num_classes', 42)
            self.num_ood_classes = metadata.get('num_ood_classes', 8)
        else:
            # Default values
            self.num_classes = 42
            self.num_ood_classes = 8
    
    def _load_data_indices(self):
        """Load and organize data indices for all modalities."""
        # Load audio datalist
        audio_csv_path = self.data_path / 'audio_datalist.csv'
        if audio_csv_path.exists():
            self.audio_df = pd.read_csv(audio_csv_path)
        else:
            raise FileNotFoundError(f"Audio datalist not found at {audio_csv_path}")
        
        # Load text data
        text_tsv_path = self.data_path / 'text_data.tsv'
        if text_tsv_path.exists():
            self.text_df = pd.read_csv(text_tsv_path, sep='\t')
        else:
            raise FileNotFoundError(f"Text data not found at {text_tsv_path}")
        
        # Load image data (EDM generated + CIFAR)
        edm_pickle_path = self.data_path / 'edm_images.pickle'
        if edm_pickle_path.exists():
            with open(edm_pickle_path, 'rb') as f:
                self.edm_df = pickle.load(f)
        else:
            warnings.warn(f"EDM images not found at {edm_pickle_path}")
            self.edm_df = None
        
        # Organize data by class and split
        self._organize_by_class()
    
    def _organize_by_class(self):
        """
        Organize data by class and create train/test splits.
        LUMA provides 500 train + 100 test samples per class.
        """
        # Get unique classes
        audio_labels = self.audio_df['label'].unique()
        text_labels = self.text_df['label'].unique() if 'label' in self.text_df.columns else audio_labels
        
        # Find common classes across modalities
        common_classes = sorted(set(audio_labels) & set(text_labels))
        
        if self.use_ood:
            self.classes = common_classes
        else:
            # Use first 42 classes as training, rest as OOD
            self.classes = common_classes[:self.num_classes]
        
        # Create label to index mapping
        self.label_to_idx = {label: idx for idx, label in enumerate(self.classes)}
        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}
        
        # Create aligned dataset
        self.samples = []
        
        for class_label in self.classes:
            class_idx = self.label_to_idx[class_label]
            
            # Get samples for this class from each modality
            audio_samples = self.audio_df[self.audio_df['label'] == class_label]
            text_samples = self.text_df[self.text_df['label'] == class_label] if 'label' in self.text_df.columns else None
            
            # Determine split
            if self.split == 'train':
                # Use first 500 samples for training
                audio_samples = audio_samples.iloc[:500]
                if text_samples is not None:
                    text_samples = text_samples.iloc[:500]
            else:  # test
                # Use last 100 samples for testing
                audio_samples = audio_samples.iloc[500:600]
                if text_samples is not None:
                    text_samples = text_samples.iloc[500:600]
            
            # Create sample tuples (audio_idx, text_idx, label)
            n_samples = len(audio_samples)
            for i in range(n_samples):
                audio_idx = audio_samples.iloc[i].name
                text_idx = text_samples.iloc[i].name if text_samples is not None else i
                
                self.samples.append({
                    'audio_idx': audio_idx,
                    'text_idx': text_idx,
                    'label': class_idx,
                    'class_name': class_label,
                })
    
    def _setup_image_transforms(self):
        """Setup image preprocessing transforms."""
        transform_list = [
            transforms.Resize(self.image_config['size']),
        ]
        
        if self.image_config.get('normalize', True):
            # CIFAR normalization
            transform_list.extend([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            transform_list.append(transforms.ToTensor())
        
        self.image_transform = transforms.Compose(transform_list)
    
    def _load_audio(self, audio_idx: int) -> torch.Tensor:
        """
        Load and process audio file.
        
        Returns:
            Tensor of shape (n_mfcc,) or (n_mels,) depending on config
        """
        audio_info = self.audio_df.iloc[audio_idx]
        audio_rel_path = audio_info['path']  # e.g., 'cv_audio/bird/103.wav'
        
        # Try to find audio file in compiled directory first
        audio_path = self.data_path / audio_rel_path
        
        # If not found, check if there's an audio_path.txt reference
        if not audio_path.exists():
            audio_path_ref = self.data_path / 'audio_path.txt'
            if audio_path_ref.exists():
                # Read the original audio directory path
                # This points to data/luma_raw/audio
                with open(audio_path_ref, 'r') as f:
                    original_audio_dir = Path(f.read().strip())
                
                # The paths in datalist are like 'cv_audio/bird/103.wav'
                # And the actual structure is: data/luma_raw/audio/cv_audio/bird/103.wav
                # So we use: original_audio_dir / audio_rel_path
                audio_path = original_audio_dir / audio_rel_path
        
        if not audio_path.exists():
            # Some audio files in the datalist might not exist
            # Return zero features as a fallback
            warnings.warn(f"Audio file not found (using zeros): {audio_path}")
            n_features = self.audio_config['n_mfcc'] if self.audio_config.get('use_mfcc') else self.audio_config.get('n_mels', 128)
            return torch.zeros(n_features)
        
        # Load audio waveform
        waveform, sample_rate = torchaudio.load(str(audio_path))
        
        # Resample if necessary
        if sample_rate != self.audio_config['sample_rate']:
            resampler = torchaudio.transforms.Resample(sample_rate, self.audio_config['sample_rate'])
            waveform = resampler(waveform)
        
        # Ensure mono
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # Truncate or pad to max_length
        max_samples = int(self.audio_config['max_length'] * self.audio_config['sample_rate'])
        if waveform.shape[1] > max_samples:
            waveform = waveform[:, :max_samples]
        elif waveform.shape[1] < max_samples:
            padding = max_samples - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, padding))
        
        # Extract features
        if self.audio_config.get('use_mfcc', True):
            # MFCC features
            # n_mels must be >= n_mfcc
            n_mfcc = self.audio_config['n_mfcc']
            n_mels = max(n_mfcc + 10, self.audio_config.get('n_mels', 128))  # Ensure n_mels > n_mfcc
            
            mfcc_transform = torchaudio.transforms.MFCC(
                sample_rate=self.audio_config['sample_rate'],
                n_mfcc=n_mfcc,
                melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': n_mels}
            )
            features = mfcc_transform(waveform)
            # Average over time dimension
            features = torch.mean(features, dim=2).squeeze(0)
        else:
            # Mel spectrogram
            mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=self.audio_config['sample_rate'],
                n_mels=self.audio_config.get('n_mels', 128)
            )
            features = mel_transform(waveform)
            # Average over time dimension
            features = torch.mean(features, dim=2).squeeze(0)
        
        return features
    
    def _load_text(self, text_idx: int) -> torch.Tensor:
        """
        Load and encode text.
        
        Returns:
            Tensor of text features
        """
        text_info = self.text_df.iloc[text_idx]
        text = text_info['text']
        
        if self.tokenizer is not None:
            # Use pretrained tokenizer
            encoding = self.tokenizer(
                text,
                max_length=self.text_config['max_length'],
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            # Return input_ids as features (we'll encode later in the model)
            return encoding['input_ids'].squeeze(0)
        else:
            # Simple word-level tokenization
            words = text.lower().split()
            # Create simple vocabulary index (this is very basic)
            # In practice, you'd want a proper vocabulary
            word_ids = [hash(word) % 10000 for word in words[:self.text_config['max_length']]]
            # Pad to max_length
            if len(word_ids) < self.text_config['max_length']:
                word_ids += [0] * (self.text_config['max_length'] - len(word_ids))
            return torch.tensor(word_ids, dtype=torch.long)
    
    def _load_image(self, label_idx: int, sample_idx: int) -> torch.Tensor:
        """
        Load and process image.
        
        For now, we create a placeholder. In practice, you'd load from CIFAR
        or EDM generated images based on the class label.
        
        Returns:
            Tensor of shape (3, H, W)
        """
        # TODO: Implement proper image loading from CIFAR/EDM
        # For now, create a random image placeholder
        if self.edm_df is not None:
            # Try to load from EDM dataframe
            class_name = self.idx_to_label[label_idx]
            # Filter EDM images for this class
            # This is a placeholder - actual implementation depends on EDM structure
            pass
        
        # Placeholder: Create a random image
        # In practice, load actual CIFAR images
        img = Image.new('RGB', self.image_config['size'], color=(128, 128, 128))
        return self.image_transform(img)
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[List[torch.Tensor], int]:
        """
        Get a multimodal sample.
        
        Returns:
            views: List of [audio_features, text_features, image_features]
            label: Class label (int)
        """
        sample = self.samples[idx]
        
        # Load each modality
        audio_features = self._load_audio(sample['audio_idx'])
        text_features = self._load_text(sample['text_idx'])
        image_features = self._load_image(sample['label'], idx)
        
        views = [audio_features, text_features, image_features]
        label = sample['label']
        
        return views, label
    
    @property
    def num_views(self) -> int:
        """Number of modalities."""
        return 3
    
    @property
    def dims(self) -> List[int]:
        """Dimension of each modality."""
        # Return approximate dimensions (will be determined at runtime)
        audio_dim = self.audio_config['n_mfcc'] if self.audio_config.get('use_mfcc') else self.audio_config.get('n_mels', 128)
        text_dim = self.text_config['max_length']
        image_dim = self.image_config['size'][0] * self.image_config['size'][1] * 3
        return [audio_dim, text_dim, image_dim]


def get_luma_dataloaders(
    data_path: str,
    audio_config: Optional[Dict] = None,
    text_config: Optional[Dict] = None,
    image_config: Optional[Dict] = None,
    batch_size: int = 64,
    num_workers: int = 4,
    train_frac: float = 0.8,
    use_ood: bool = False,
) -> Tuple[DataLoader, DataLoader, int, int, List[int]]:
    """
    Create train and test dataloaders for LUMA dataset.
    
    Args:
        data_path: Path to compiled LUMA dataset
        audio_config: Audio processing configuration
        text_config: Text processing configuration
        image_config: Image processing configuration
        batch_size: Batch size for dataloaders
        num_workers: Number of worker processes
        train_frac: Fraction of data to use for training (ignored, uses predefined split)
        use_ood: Whether to include OOD samples
    
    Returns:
        train_loader: Training dataloader
        test_loader: Testing dataloader
        num_classes: Number of classes
        num_views: Number of modalities (always 3 for LUMA)
        dims: List of dimensions for each modality
    """
    # Create datasets
    train_dataset = LUMADataset(
        data_path=data_path,
        split='train',
        audio_config=audio_config,
        text_config=text_config,
        image_config=image_config,
        use_ood=use_ood,
    )
    
    test_dataset = LUMADataset(
        data_path=data_path,
        split='test',
        audio_config=audio_config,
        text_config=text_config,
        image_config=image_config,
        use_ood=use_ood,
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    num_classes = train_dataset.num_classes
    num_views = train_dataset.num_views
    dims = train_dataset.dims
    
    return train_loader, test_loader, num_classes, num_views, dims