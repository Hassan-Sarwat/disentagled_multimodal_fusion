"""
LUMA Dataset Loader for Multimodal Uncertainty Quantification

This module provides PyTorch Dataset and DataLoader functionality for the LUMA dataset,
which contains audio, text, and image modalities.

LUMA consists of:
- Audio: wav files of people pronouncing class labels
- Text: Short passages about class labels (generated by LLMs)
- Image: Images from CIFAR-10/100 subset

Dataset structure after compilation:
- 42 training classes, 8 OOD classes
- 500 samples per class (train), 100 samples per class (test)
"""

import os
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
from typing import Tuple, List, Optional, Dict
import warnings

import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchvision.transforms as transforms
from PIL import Image

# Try to import transformers for text encoding
try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    warnings.warn("transformers library not available. Text encoding will use simple tokenization.")


class LUMADataset(Dataset):
    """
    LUMA Multimodal Dataset for Uncertainty Quantification.
    
    Loads and provides access to audio, text, and image modalities.
    
    Returns data in the same format as MultiViewDataset from dataset.py:
    __getitem__ returns a flat list: [modal0, modal1, modal2, label]
    """
    
    def __init__(
        self,
        data_path: str,
        split: str = 'train',
        audio_config: Optional[Dict] = None,
        text_config: Optional[Dict] = None,
        image_config: Optional[Dict] = None,
        use_ood: bool = False,
    ):
        """
        Initialize LUMA dataset.
        
        Args:
            data_path: Path to compiled LUMA dataset
            split: 'train' or 'test'
            audio_config: Audio processing configuration
            text_config: Text processing configuration
            image_config: Image processing configuration
            use_ood: Whether to include out-of-distribution samples
        """
        self.data_path = Path(data_path)
        self.split = split
        self.use_ood = use_ood
        
        # Default configurations
        self.audio_config = audio_config or {
            'sample_rate': 16000,
            'max_length': 3.0,
            'n_mfcc': 40,
            'use_mfcc': True,
        }
        
        self.text_config = text_config or {
            'max_length': 128,
            'model_name': 'bert-base-uncased',
            'use_pretrained': TRANSFORMERS_AVAILABLE,
        }
        
        self.image_config = image_config or {
            'size': (32, 32),
            'normalize': True,
        }
        
        # Load dataset metadata
        self._load_metadata()
        
        # Load data indices and labels
        self._load_data_indices()
        
        # Initialize text tokenizer if available
        if self.text_config['use_pretrained'] and TRANSFORMERS_AVAILABLE:
            self.tokenizer = AutoTokenizer.from_pretrained(self.text_config['model_name'])
        else:
            self.tokenizer = None
        
        # Setup image transforms
        self._setup_image_transforms()
        
        print(f"Loaded LUMA {split} dataset: {len(self)} samples, {self.num_classes} classes")
    
    def _load_metadata(self):
        """Load dataset metadata."""
        metadata_path = self.data_path / 'metadata.yaml'
        
        if metadata_path.exists():
            import yaml
            with open(metadata_path, 'r') as f:
                metadata = yaml.safe_load(f)
            
            self.num_classes = metadata.get('num_classes', 42)
            self.num_ood_classes = metadata.get('num_ood_classes', 8)
        else:
            # Default values
            self.num_classes = 42
            self.num_ood_classes = 8
    
    def _load_data_indices(self):
        """Load and organize data indices for all modalities."""
        # Load audio datalist
        audio_csv_path = self.data_path / 'audio_datalist.csv'
        if audio_csv_path.exists():
            self.audio_df = pd.read_csv(audio_csv_path)
        else:
            raise FileNotFoundError(f"Audio datalist not found at {audio_csv_path}")
        
        # Load text data
        text_tsv_path = self.data_path / 'text_data.tsv'
        if text_tsv_path.exists():
            self.text_df = pd.read_csv(text_tsv_path, sep='\t')
        else:
            raise FileNotFoundError(f"Text data not found at {text_tsv_path}")
        
        # Load image data (EDM generated + CIFAR)
        edm_pickle_path = self.data_path / 'edm_images.pickle'
        if edm_pickle_path.exists():
            # This file contains all image data, not just EDM.
            self.image_df = pd.read_pickle(edm_pickle_path)
        else:
            warnings.warn(f"EDM images not found at {edm_pickle_path}")
            self.image_df = None
        
        # Organize data by class and split
        self._organize_by_class()
    
    def _organize_by_class(self):
        """
        Organize data by class and create train/test splits.
        LUMA provides 500 train + 100 test samples per class.
        """
        # Get unique classes
        audio_labels = self.audio_df['label'].unique()
        if 'label' in self.text_df.columns:
            text_labels = self.text_df['label'].unique()
        else:
            # Fallback if text_data.tsv has no labels
            text_labels = audio_labels
        
        # Find common classes across modalities
        common_classes = sorted(set(audio_labels) & set(text_labels))
        
        if self.use_ood:
            self.classes = common_classes
        else:
            # Use first 42 classes as training, rest as OOD
            self.classes = common_classes[:self.num_classes]
        
        # Create label to index mapping
        self.label_to_idx = {label: idx for idx, label in enumerate(self.classes)}
        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}
        
        # Create aligned dataset
        self.samples = []
        
        for class_label in self.classes:
            class_idx = self.label_to_idx[class_label]
            
            # Get samples for this class from each modality
            audio_samples = self.audio_df[self.audio_df['label'] == class_label]
            text_samples = self.text_df[self.text_df['label'] == class_label] if 'label' in self.text_df.columns else None
            image_samples = self.image_df[self.image_df['label'] == class_label] if self.image_df is not None else None
            
            # Determine split
            if self.split == 'train':
                # Use first 500 samples for training
                audio_samples = audio_samples.iloc[:500]
                if text_samples is not None:
                    text_samples = text_samples.iloc[:500]
                if image_samples is not None:
                    image_samples = image_samples.iloc[:500]
            else:  # test
                # Use last 100 samples for testing
                audio_samples = audio_samples.iloc[500:600]
                if text_samples is not None:
                    text_samples = text_samples.iloc[500:600]
                if image_samples is not None:
                    image_samples = image_samples.iloc[500:600]
            
            # Create sample tuples (audio_idx, text_idx, label)
            n_samples = len(audio_samples)
            for i in range(n_samples):
                audio_idx = audio_samples.iloc[i].name
                text_idx = text_samples.iloc[i].name if text_samples is not None else i
                image_idx = image_samples.iloc[i].name if image_samples is not None else -1

                self.samples.append({
                    'audio_idx': audio_idx,
                    'text_idx': text_idx,
                    'label': class_idx,
                    'class_name': class_label,
                })
    
    def _setup_image_transforms(self):
        """Setup image preprocessing transforms."""
        transform_list = [
            transforms.Resize(self.image_config['size']),
        ]
        
        if self.image_config.get('normalize', True):
            # CIFAR normalization
            transform_list.extend([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            transform_list.append(transforms.ToTensor())
        
        self.image_transform = transforms.Compose(transform_list)
    
    def _load_audio(self, audio_idx: int) -> torch.Tensor:
        """
        Load and process audio file.
        
        Returns:
            Tensor of audio features (MFCC or mel spectrogram)
        """
        audio_info = self.audio_df.iloc[audio_idx]
        filepath = Path(audio_info['filepath'])
        
        # Handle both absolute and relative paths in the datalist
        if filepath.is_absolute():
            audio_path = filepath
        else:
            audio_path = self.data_path / filepath
        
        # Load audio
        waveform, sample_rate = torchaudio.load(audio_path)
        
        # Resample if necessary
        if sample_rate != self.audio_config['sample_rate']:
            resampler = torchaudio.transforms.Resample(sample_rate, self.audio_config['sample_rate'])
            waveform = resampler(waveform)
        
        # Ensure mono
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # Trim or pad to max_length
        target_length = int(self.audio_config['max_length'] * self.audio_config['sample_rate'])
        if waveform.shape[1] > target_length:
            waveform = waveform[:, :target_length]
        elif waveform.shape[1] < target_length:
            padding = target_length - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, padding))
        
        # Extract features
        if self.audio_config.get('use_mfcc', True):
            # MFCC features
            mfcc_transform = torchaudio.transforms.MFCC(
                sample_rate=self.audio_config['sample_rate'],
                n_mfcc=self.audio_config['n_mfcc'],
                melkwargs={'n_mels': 40, 'n_fft': 400}
            )
            features = mfcc_transform(waveform)
            # Average over time dimension
            features = torch.mean(features, dim=2).squeeze(0)
        else:
            # Mel spectrogram
            mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=self.audio_config['sample_rate'],
                n_mels=self.audio_config.get('n_mels', 128)
            )
            features = mel_transform(waveform)
            # Average over time dimension
            features = torch.mean(features, dim=2).squeeze(0)
        
        return features
    
    def _load_text(self, text_idx: int) -> torch.Tensor:
        """
        Load and encode text.
        
        Returns:
            Tensor of text features (simple embedding, not pretrained BERT)
        """
        text_info = self.text_df.iloc[text_idx]
        text = text_info['text']
        
        # For DMVAE compatibility, we need to return a simple float vector
        # NOT token IDs that need embedding layers
        if self.tokenizer is not None:
            # Use pretrained tokenizer to get token IDs
            encoding = self.tokenizer(
                text,
                max_length=self.text_config['max_length'],
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            # Convert token IDs to simple features (normalized to [0,1])
            token_ids = encoding['input_ids'].squeeze(0).float()
            # Normalize to reasonable range for DMVAE
            token_features = token_ids / self.tokenizer.vocab_size
            return token_features
        else:
            # Simple word-level tokenization
            words = text.lower().split()
            # Create simple vocabulary index
            word_ids = [hash(word) % 10000 for word in words[:self.text_config['max_length']]]
            # Pad to max_length
            if len(word_ids) < self.text_config['max_length']:
                word_ids += [0] * (self.text_config['max_length'] - len(word_ids))
            # Normalize to [0, 1] range
            word_features = torch.tensor(word_ids, dtype=torch.float32) / 10000.0
            return word_features
    
    def _load_image(self, label_idx: int, sample_idx: int) -> torch.Tensor:
        """
        Load and process image.
        
        For now, we create a placeholder. In practice, you'd load from CIFAR
        or EDM generated images based on the class label.
        
        Returns:
            Tensor of shape (H*W*C,) - flattened image
        """
        sample_info = self.samples[sample_idx]
        image_idx = sample_info.get('image_idx', -1)

        if self.image_df is not None and image_idx != -1:
            # Load image from the DataFrame
            # The 'image' column contains numpy arrays of shape (32, 32, 3)
            img_array = self.image_df.loc[image_idx, 'image']
            img = Image.fromarray(img_array)
        else:
            # Fallback to a placeholder if image data is missing
            warnings.warn(f"Image for sample {sample_idx} not found, using placeholder.")
            img = Image.new('RGB', self.image_config['size'], color=(128, 128, 128))

        img_tensor = self.image_transform(img)  # Shape: (3, H, W)
        
        # Flatten to (C*H*W,) for the model
        img_flat = img_tensor.flatten()
        
        return img_flat
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int):
        """
        Get a multimodal sample.
        
        Returns a flat list: [audio_features, text_features, image_features, label]
        This matches the structure of MultiViewDataset in dataset.py
        """
        sample = self.samples[idx]
        
        # Load each modality
        audio_features = self._load_audio(sample['audio_idx'])
        text_features = self._load_text(sample['text_idx'])
        image_features = self._load_image(sample['label'], idx)
        
        label = sample['label']
        
        # CRITICAL: Return as flat list, not tuple
        # This matches MultiViewDataset.__getitem__ behavior
        # Each feature is already a tensor, label is int
        return [
            audio_features.float(),   # Shape: (n_mfcc,)
            text_features.float(),    # Shape: (max_length,)
            image_features.float(),   # Shape: (3*H*W,)
            label                     # int
        ]
    
    @property
    def num_views(self) -> int:
        """Number of modalities."""
        return 3
    
    @property
    def dims(self) -> np.ndarray:
        """
        Dimension of each modality.
        Returns numpy array for compatibility with dataset.py
        """
        audio_dim = self.audio_config['n_mfcc'] if self.audio_config.get('use_mfcc') else self.audio_config.get('n_mels', 128)
        text_dim = self.text_config['max_length']
        image_dim = self.image_config['size'][0] * self.image_config['size'][1] * 3
        
        # Return as numpy array with shape (3, 1) to match MultiViewDataset.get_dims()
        return np.array([[audio_dim], [text_dim], [image_dim]])


def get_luma_dataloaders(
    data_path: str,
    audio_config: Optional[Dict] = None,
    text_config: Optional[Dict] = None,
    image_config: Optional[Dict] = None,
    batch_size: int = 64,
    num_workers: int = 4,
    train_frac: float = 0.8,
    use_ood: bool = False,
) -> Tuple[DataLoader, DataLoader, int, int, np.ndarray]:
    """
    Create train and test dataloaders for LUMA dataset.
    
    Args:
        data_path: Path to compiled LUMA dataset
        audio_config: Audio processing configuration
        text_config: Text processing configuration
        image_config: Image processing configuration
        batch_size: Batch size for dataloaders
        num_workers: Number of worker processes
        train_frac: Fraction of data to use for training (ignored, uses predefined split)
        use_ood: Whether to include OOD samples
    
    Returns:
        train_loader: Training dataloader
        test_loader: Testing dataloader
        num_classes: Number of classes
        num_views: Number of modalities (always 3 for LUMA)
        dims: Array of dimensions for each modality
    """
    # Create datasets
    train_dataset = LUMADataset(
        data_path=data_path,
        split='train',
        audio_config=audio_config,
        text_config=text_config,
        image_config=image_config,
        use_ood=use_ood,
    )
    
    test_dataset = LUMADataset(
        data_path=data_path,
        split='test',
        audio_config=audio_config,
        text_config=text_config,
        image_config=image_config,
        use_ood=use_ood,
    )
    
    # Create dataloaders WITHOUT custom collate_fn
    # The default collate_fn will work correctly with our flat list structure
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    num_classes = train_dataset.num_classes
    num_views = train_dataset.num_views
    dims = train_dataset.dims
    
    return train_loader, test_loader, num_classes, num_views, dims